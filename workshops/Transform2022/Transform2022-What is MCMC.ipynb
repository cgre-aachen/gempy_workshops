{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![gempyterranigma](images/gempynigma.png)\n",
    "# Transform 2022 - What is Markov chain Monte Carlo?\n",
    "<a id=\"0\"></a>\n",
    "Monday - 25.04.2022\n",
    "\n",
    "\n",
    "## Table of Content\n",
    "\n",
    "* [Basic Introduction - Who is Markov and what do they do in Monte Carlo?](#1)  \n",
    "* [How can we use MCMC in geosciences?](#2)\n",
    "* [MCMC in Python - Introduction to PyMC3](#3)  \n",
    "    * [Model definition](#3.1)  \n",
    "    * [Simplest model](#3.2)  \n",
    "    * [One Observation](#3.3)  \n",
    "    * [Increasing the number of observations - sampling](#3.4)\n",
    "* [Extending the (spatial) dimension](#4)\n",
    "* [Applying this to a geological model](#5)\n",
    "* [Bayesian Inference with gempy and pymc3](#6)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "## Basic Introduction - Who is Markov and what do they do in Monte Carlo?\n",
    "Markov Chain Monte Carlo, often abbreviated _MCMC_, is a method to sample the distribution of a parameter in a probabilistic space. Like Monte Carlo, it randomly samples, but unlike pure Monte Carlo, it makes an educated guess _where_ to sample (from iteration to iteration). And unlike pure Monte Carlo, these guesses get better with progressing iterations. Plus, a Markov Chain is reversible, has a memory so to say. So you can trace back your \"jumps\" in probability space.  \n",
    "\n",
    "<hr>\n",
    "<center>\n",
    "<img src=\"https://images.unsplash.com/photo-1595138320174-a64d168e9970?ixlib=rb-1.2.1&ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&auto=format&fit=crop&w=2060&q=80\" width=\"620\"></img></center>\n",
    "Monte Carlo photo by <a href=\"https://unsplash.com/@rishi_1?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">Rishi Jhajharia</a> on <a href=\"https://unsplash.com/s/photos/monte-carlo?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">Unsplash</a>\n",
    "<hr>\n",
    "\n",
    "Or to quote [Thomas Wiecki](https://twiecki.io/blog/2015/11/10/mcmc-sampling/):\n",
    "<center>\"Well that's easy, MCMC generates samples from the posterior distribution by constructing a reversible Markov-chain that has as its equilibrium distribution the target posterior distribution. Questions?\"</center>\n",
    "<br>\n",
    "\n",
    "It should be noted, that Thomas immediately questions the usefullness of this statement. He then provides an extensive explanation of the underlying _intuition_ of MCMC, accompanied by a lot of Python code! https://twiecki.io/blog/2015/11/10/mcmc-sampling/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "## How can we use MCMC in geosciences?  \n",
    "\n",
    "Applications of MCMC in geosciences are manifold. Simply, because knowing the `evidence` is almost never the case. Thus, MCMC can be used for estimating a parameter's posterior without knowing the `evidence`, also called the normalizing constant [2]. Most often, this is done by having some observations to begin with and formulating a forward problem. Parameters of this problem are assigned to distributions. The forward problem is then solved iteratively with parameter values sampled from the distributions until a set of parameter values is reached, whose equilibrium distribution is similar to the posterior.  \n",
    "\n",
    "Among others, some applications are:  \n",
    "\n",
    "* Location of earthquake hypocenters  \n",
    "* Uncertainty quantification in geological modeling  \n",
    "* Geochronology  \n",
    "* Inferring environmental variables (sea-level, sediment supply, ...) from stratigraphic piles  \n",
    "* _and many more_  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "## MCMC in Python - Introduction to PyMC3\n",
    "\n",
    "To perform Bayesian inference via MCMC, PyMC3 was developed. It is an open source framework for probabilistic programming building on Theano (for AD and speed increase). PyMC3 is quite high-level, meaning its syntax is \"close to the natural syntax statisticians use to describe models\" [3]. In addition to a quite high-level and approachable syntax, PyMC3 features different kinds of sampling algorithms in MCMC, e.g. NUTS or HMC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # These two lines are necessary only if GemPy is not installed\n",
    "import sys, os\n",
    "\n",
    "sys.path.insert(0, \"../../../gempy/\")\n",
    "os.environ[\"THEANO_FLAGS\"] = \"mode=FAST_RUN,device=cpu\"\n",
    "\n",
    "# Importing GemPy\n",
    "import gempy as gp\n",
    "\n",
    "# Embedding matplotlib figures in the notebooks\n",
    "%matplotlib inline\n",
    "\n",
    "# Importing auxiliary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pymc3 as pm\n",
    "import arviz as az\n",
    "\n",
    "from gempy.bayesian import plot_posterior as pp\n",
    "from gempy.bayesian.plot_posterior import default_blue, default_red\n",
    "import seaborn as sns\n",
    "from importlib import reload\n",
    "from matplotlib.ticker import StrMethodFormatter\n",
    "\n",
    "sns.set_style('white')\n",
    "sns.set_context('talk')\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3.1\"></a>\n",
    "### Model definition\n",
    "Generally, models represent an abstraction of reality to answer a specific question, to fulfill a certain purpose, or to _simulate_ (mimic) a proces or multiple processes. What models share is the aspiration to be as realistic as possible, so they can be used for prognoses and to better understand a real-world system.\n",
    "\n",
    "Fitting of these models to acquired measurements or observations is called calibration and a standard procedure for improving a models reliability (**to answer the question it was designed for**). \n",
    "\n",
    "Models can also be seen as a general descriptor of correlation of observations in multiple dimensions. Complex systems with generally sparse data coverage (e.g. the subsurface) are difficult to reliably encode from the real-world in the numerical abstraction, i.e. a computational model.\n",
    "\n",
    "In a probabilistic framework, a model is a framework of different input distributions, which, as an output, has another probability distribution. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the notebook, we will work with a couple of observations (`y_obs`, `y_obs_list`) of a property encapsulated in a model. Depending on its purpose, these may be anything from petrophysical properties of rocks (density, thermal conductivity, porosity, ... you name it), to thickness of geological layers, and honestly any model property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(4003)\n",
    "thickness_observation = [2.12]\n",
    "thickness_observation_list = [2.12, 2.06, 2.08, 2.05, 2.08, 2.09,\n",
    "                              2.19, 2.07, 2.16, 2.11, 2.13, 1.92]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those observations are used for generating Distributions (~ probabilistic models) in [PyMC3](https://docs.pymc.io/en/v3/) which we encapsulate in the following function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3.2\"></a>\n",
    "### Simplest probabilistic modeling\n",
    "\n",
    "Consider the simplest probabilistic model where the output $y$ of a model is a distribution. Let's assume, $y$ is a normal distribution, described by a mean $\\mu$ and a standard deviation $\\sigma$. Usually, those are considered scalar values, but they themselves can be distributions. This will yield a change of the width and position of the normal distribution $y$ with each iteration.\n",
    "\n",
    "As a reminder, a normal distribution is defined as: \n",
    "\n",
    "$$ y = \\frac{1}{\\sigma \\sqrt{2\\pi}} \\, e^{-\\frac{(x - \\mu)^2}{2 \\sigma ^2}} $$\n",
    "\n",
    "* $\\mu$ mean (Normal distribution)  \n",
    "* $\\sigma$ standard deviation (Gamma distribution, Gamma log-likelihood)  \n",
    "* $y$ Normal distribution\n",
    "\n",
    "With this constructed model, we are able to infer which model parameters will fit observations better by _optimizing_ for regions with high density mass. In addition (or even substituting) to data observations, informative values like prior simulations or expert knowledge can pour into the construction of the first $y$ distribution, the _prior_.  \n",
    "\n",
    "There isn't a limitation about how \"informative\" a prior can or must be. Depending on the variance of the model's parameters and on the number of observations, a model will be more _prior driven_ or _data driven_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set up a `pymc3` model using the `thickness_observation` from above as observations and with $\\mu$ and $\\sigma$ being:  \n",
    "* $\\mu$ = Normal distribution with mean 2.08 and standard deviation 0.07  \n",
    "* $\\sigma$ = Gamma distribution with $\\alpha$ (shape parameter) 0.3 and $\\beta$ (rate parameter) 3  \n",
    "* $y$ = Normal distribution with $\\mu$, $\\sigma$ and `thickness_observation_list` as observations\n",
    "\n",
    "A [Gamma distribution](https://docs.pymc.io/en/latest/api/distributions/generated/pymc.Gamma.html) can also be expressed by mean and standard deviation with $\\alpha = \\frac{\\mu^2}{\\sigma^2}$ and $\\beta = \\frac{\\mu}{\\sigma^2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "with pm.Model() as model_single_point:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.model_to_graphviz(model_single_point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3.3'></a>\n",
    "###  One Observation:\n",
    "\n",
    "Before diving in sampling, let's look at a model, where we have a single observation to sample the posterior from a prior with a normal distribution for $\\mu$ and a gamma distribution for $\\sigma$: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample model    \n",
    "with model_single_point:\n",
    "\n",
    "\n",
    "# retrieve data\n",
    "data1 = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the prior\n",
    "p1 = pp.PlotPosterior(data1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# plot the posterior\n",
    "\n",
    "# Define function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Plot range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "MCMC boils down to be a collection of method helping to do bayesian inference, thus based on Bayes Theorem:  \n",
    "\n",
    "$$P(\\theta | x) = \\frac{P(x|\\theta) P(\\theta)}{P(x)} $$  \n",
    "\n",
    "* $P(\\theta | x)$ is the Posterior  \n",
    "* $P(x)$ is the Prior  \n",
    "* $P(x | \\theta)$ is the Likelihood  \n",
    "* $P(x)$ the evidence  \n",
    "\n",
    "As calculating the posterior in this form is most likely not possible in real-world problems. If one could sample from the posterior, one might approximate it with Monte Carlo. But in order to sample directly from the posterior, one would need to invert Bayes Theorem.  \n",
    "\n",
    "The solution to this problem is, when we cannot draw MC (in this case Monte Carlo) samples from the distribution directly, we let an MC (now a Markov Chain) do it for us. [1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3.4\"></a>\n",
    "## What con we do next? Increasing the number of observations - sampling\n",
    "We previously mentioned that we can optimize this model to better fit observations. For this, we generate a prior from the model we previously created, the one with multiple observations from `y_obs_list`.\n",
    "In the following, samples are iteratively drawn and evaluated to form the posterior. This generates what is called a \"trace\". A predictive Posterior distribution is then generated from said trace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "with pm.Model() as model_multiple_points:\n",
    "    \n",
    "# Back to the model with several observations\n",
    "with model_multiple_points:\n",
    "    \n",
    "# retrieve data\n",
    "data2 = az.from_pymc3(trace=trace,\n",
    "                      prior=prior,\n",
    "                      posterior_predictive=post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Raw observations:\n",
    "The behaviour of this chain is controlled by the observations we fed into the model. Let's have a look again at the observations and how they are spread/distributed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2 = pp.PlotPosterior(data2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bulk of observations is between 2.05 and 2.15, one observation at the lower end with 1.92."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final inference  \n",
    "\n",
    "Now let's plot the inferred posterior distribution (i.e. the last sample iteration) and the observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p3 = pp.PlotPosterior(data2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bell-peak is above a cluster of observations, but the one observation at 1.92 seems to be of greater importance, as otherwise, the highest likelihood might be located more around 2.1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joyplot\n",
    "\n",
    "Just plotting the posterior does not convey the underlying process. Attached to GemPy, and used for its stochastic capabilities, visualization methods for drawing distributions were written. More specifically, we use joyplots for showing the change of the distribution (more precisely its mean and its standard deviation) with the progressing chaing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(pp)\n",
    "joy = pp.PlotPosterior(data2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we show a gif of how the curve actually moves ($\\mu$ is changed) and changes its width (change in $\\sigma$ parameters) with progressive sampling. Dark colors represent an increase in likeliness. However, we see that the colors change with new iterations. That means, that what previously seemed likely becomes less likely as we keep exploring the probability space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "![joyplot animated](images/joyplot_2.gif-REMOVETHIS \"segment\")\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(pp)\n",
    "p23 = pp.PlotPosterior(data2)\n",
    "\n",
    "p23.create_figure(figsize=(18, 10), textsize=15, joyplot=False, marginal=True, likelihood=True)\n",
    "p23.plot_marginal(var_names=['$\\mu$', '$\\sigma$'],\n",
    "                  plot_trace=False, credible_interval=.93, kind='kde')\n",
    "\n",
    "p23.plot_normal_likelihood('$\\mu$', '$\\sigma$', '$y$', iteration=-1, hide_lines=True)\n",
    "p23.likelihood_axes.set_xlim(1.70, 2.40);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The small gif below shows the first 100 samplings (starting from the 10th iteration) of the chain:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "![sampling](images/sampling_2.gif-REMOVETHIS \"segment\")\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sped-up version of the full sampling (each 10 steps until we reach the 1000th iteration) provides an impression how we arrive at the posterior distribution...and that the change after the first couple of iterations gets smaller and smaller, as the chain itself gets more and more \"educated\" about its guesses:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "![sampling](images/sampling.gif-REMOVETHIS \"segment\")\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p3 = pp.PlotPosterior(data2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending the Dimensions \n",
    "\n",
    "In the model/example before, we had mean and standard deviation of a parameter $y$, which all were uncertain. Let's take this a step further and say, we look at a layered sequence, so we have a top contact `y_top`, a bottom contact `y_bottom` and a resulting thickness `y`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thickness_observation = [2.12]\n",
    "thickness_observation_list = [2.12, 2.06, 2.08, 2.05, 2.08, 2.09,\n",
    "                              2.19, 2.07, 2.16, 2.11, 2.13, 1.92]\n",
    "\n",
    "np.random.seed(4003)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining some observations (yeah they might be small for layer coordinates, but just think of them as \"decameters\"), let's set up the model. Now, we actually have three \"groups\" of probabilistic models, one for each contact and one for the thickness. And on top, mean and standard deviation of contacts and thickness are also distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model_layers:\n",
    "    # Define priors for top\n",
    "    \n",
    "    \n",
    "    y_top = pm.Normal('y_{top}', mu=mu_top, sd=sigma_top, observed=[3.02])\n",
    "\n",
    "    # Define priors for bottom\n",
    "    \n",
    "    \n",
    "    y_bottom = pm.Normal('y_{bottom}', mu=mu_bottom, sd=sigma_bottom, observed=[1.02])\n",
    "\n",
    "    # Define priors for thickness\n",
    "    \n",
    "    \n",
    "    y = pm.Normal('y_{thickness}', mu=mu_thick, sd=sigma_thick, observed=thickness_observation_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This yields a suddenly more complex graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.model_to_graphviz(model_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's sample from the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model_layers:\n",
    "    prior = pm.sample_prior_predictive(500)\n",
    "    trace = pm.sample(500, discard_tuned_samples=False, chains=1, cores=1)\n",
    "    post = pm.sample_posterior_predictive(trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = az.from_pymc3(trace=trace,\n",
    "                     prior=prior,\n",
    "                     posterior_predictive=post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_density(..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we see a correlation between the posteriors of $\\mu_{bottom}$ and $\\mu_{top}$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p3 = pp.PlotPosterior(data)\n",
    "\n",
    "p3.create_figure(figsize=(15, 13), joyplot=False, marginal=True, likelihood=False)\n",
    "p3.plot_marginal(var_names=['$\\\\mu_{top}$', '$\\\\mu_{bottom}$'],\n",
    "                 plot_trace=False, credible_interval=.70, kind='kde' );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How did we get to this plot? Let's observe the Markov Chain a bit, doing its sampling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "from importlib import reload\n",
    "\n",
    "reload(pp)\n",
    "p3 = pp.PlotPosterior(data)\n",
    "p3.create_figure(figsize=(15, 9), joyplot=True)\n",
    "\n",
    "\n",
    "def change_iteration(iteration):\n",
    "    p3.plot_posterior(['$\\\\mu_{top}$', '$\\\\mu_{bottom}$'],\n",
    "                      ['$\\mu_{thickness}$', '$\\sigma_{thickness}$'],\n",
    "                      'y_{thickness}', iteration, marginal_kwargs={\"credible_interval\": 0.94})\n",
    "\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n",
    "interact(change_iteration, iteration=widgets.IntSlider(min=0, max=100, step=1, value=10, continuous_update=False));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5\"></a>\n",
    "## Applying this to geological modeling\n",
    "\n",
    "In the previous example we assume constant thickness to be able to reduce the problem to one dimension. This keeps the probabilistic model fairly simple since we do not need to deel with complex geometric structures. Unfortunaly, geology is all about dealing with complex three dimensional structures. In the moment data spread across the physical space, the probabilistic model will have to expand to relate data from different locations. In other words, the model will need to include either interpolations, regressions or some other sort of spatial functions. In this paper, we use an advance universal co-kriging interpolator. Further implications of using this method will be discuss below but for this lets treat is a simple spatial interpolation in order to keep the focus on the constraction of the probabilistic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Set up gempy model\n",
    "\n",
    "# create new model\n",
    "geo_model = gp.create_model('2-layers')\n",
    "gp.init_data(geo_model, extent=[0, 12e3, -2e3, 2e3, 0, 4e3], resolution=[100, 1, 100])\n",
    "\n",
    "# Define formations\n",
    "geo_model.add_surfaces('surface 1')\n",
    "geo_model.add_surfaces('surface 2')\n",
    "geo_model.add_surfaces('basement')\n",
    "\n",
    "# Map values to formations\n",
    "dz = geo_model.grid.regular_grid.dz  # this is the thickness of a cell\n",
    "geo_model.add_surface_values([dz, 0, 0], 'dz')\n",
    "geo_model.add_surface_values([2.6, 2.4, 3.2], 'density')\n",
    "\n",
    "# Add geometric input \n",
    "geo_model.add_surface_points(3e3, 0, 3.05e3, 'surface 1')\n",
    "geo_model.add_surface_points(9e3, 0, 3.05e3, 'surface 1')\n",
    "\n",
    "geo_model.add_surface_points(3e3, 0, 1.02e3, 'surface 2')\n",
    "geo_model.add_surface_points(9e3, 0, 1.02e3, 'surface 2')\n",
    "\n",
    "geo_model.add_orientations(6e3, 0, 4e3, 'surface 1', [0, 0, 1])\n",
    "\n",
    "# Set gravity device\n",
    "device_loc = np.array([[6e3, 0, 4e3]])\n",
    "geo_model.set_centered_grid(device_loc, radius=4000, resolution=[10, 10, 30])\n",
    "\n",
    "# Compile mode\n",
    "gp.set_interpolator(geo_model,\n",
    "                    output=['gravity'], pos_density=2,\n",
    "                    gradient=False,\n",
    "                    theano_optimizer='fast_run')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Plotting input of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p3 = gp.plot.plot_2d(geo_model, show_data=True, show_results=False, figsize=(18, 10))  #plot_data(geo_model)\n",
    "p3.fig.set_figwidth(20)\n",
    "p3.fig.axes[-1].scatter([3e3], [4e3], marker='^', s=200, c='k', zorder=10)\n",
    "p3.fig.axes[-1].scatter([9e3], [4e3], marker='^', s=200, c='k', zorder=10)\n",
    "p3.fig.axes[-1].scatter([6e3], [4e3], marker='x', s=400, c='red', zorder=10)\n",
    "\n",
    "p3.fig.axes[-1].vlines(3e3, .5e3, 10e3, linewidth=4, color='gray', )\n",
    "p3.fig.axes[-1].vlines(9e3, .5e3, 10e3, linewidth=4, color='gray')\n",
    "p3.fig.axes[-1].vlines(3e3, .5e3, 10e3)\n",
    "p3.fig.axes[-1].vlines(9e3, .5e3, 10e3)\n",
    "p3.fig.axes[-1].vlines(9e3, .5e3, 10e3)\n",
    "p3.fig.axes[-1].fill_between(x=[5.5e3, 6.5e3], y1=[0, 0], y2=[10e3, 10e3], alpha=.7, color=default_blue);\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp.plot_2d(geo_model, direction='z', figsize=(18, 10))\n",
    "plt.scatter(device_loc[:, 0], device_loc[:, 1], s=300, marker='x', c='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Interpolating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sol = gp.compute_model(geo_model, set_solutions=True, compute_mesh=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "p3 = gp.plot_2d(geo_model, cell_number=0, show_data=True, direction='y')\n",
    "p3.fig.set_figwidth(20)\n",
    "p3.fig.axes[-1].scatter([3e3], [3.9e3], marker='^', s=200, c='k', zorder=10)\n",
    "p3.fig.axes[-1].scatter([9e3], [3.9e3], marker='^', s=200, c='k', zorder=10)\n",
    "p3.fig.axes[-1].scatter([6e3], [4e3], marker='x', s=400, c='red', zorder=10)\n",
    "\n",
    "p3.fig.axes[-1].vlines(3e3, .5e3, 10e3, linewidth=4, color='gray', )\n",
    "p3.fig.axes[-1].vlines(9e3, .5e3, 10e3, linewidth=4, color='gray')\n",
    "p3.fig.axes[-1].vlines(3e3, .5e3, 10e3)\n",
    "p3.fig.axes[-1].vlines(9e3, .5e3, 10e3)\n",
    "p3.fig.axes[-1].vlines(9e3, .5e3, 10e3)\n",
    "p3.fig.axes[-1].fill_between(x=[5.5e3, 6.5e3], y1=[0, 0], y2=[10e3, 10e3], alpha=.7, color=default_blue);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_model.solutions.fw_gravity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "geo_model.solutions.block_matrix[0][1].reshape(100, 1, 100)[25, 0, :].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(geo_model.solutions.block_matrix[0][1].reshape(100, 1, 100)[:, 0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset\n",
    "grav__ = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_section(z1, z2):\n",
    "    geo_model.modify_surface_points([2, 3], Z=z1)\n",
    "    geo_model.modify_surface_points([0, 1], Z=z2)\n",
    "    gp.compute_model(geo_model, output='gravity', compute_mesh=False)\n",
    "    grav__.append(geo_model.solutions.fw_gravity)\n",
    "    print(grav__)\n",
    "    fig = gp.plot_2d(geo_model, cell_number=0, show_data=True, direction='y')\n",
    "    plt.scatter(geo_model.grid.centered_grid.values[:, 0], geo_model.grid.centered_grid.values[:, 2],\n",
    "                c='k', s=3)\n",
    "    plt.show()\n",
    "    plt.plot(grav__, 'o')\n",
    "    plt.ylabel('grav')\n",
    "    plt.show()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n",
    "interact(plot_section,\n",
    "         z1=widgets.IntSlider(min=2.7e3, max=3.5e3, step=100, continuous_update=False),\n",
    "         z2=widgets.IntSlider(min=.8e3, max=1.6e3, step=100, continuous_update=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"6\"></a>\n",
    "## Bayesian Inference with GemPy and Pymc3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_bool = geo_model.surface_points.df['surface'].isin(['surface 1', 'surface 2'])\n",
    "indices = geo_model.surface_points.df.index[indices_bool]\n",
    "Z_init = geo_model.surface_points.df.loc[indices, 'Z'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import theano.tensor as tt\n",
    "\n",
    "\n",
    "def compute_gempy_model(Z_pos):\n",
    "    geo_model.modify_surface_points(indices, Z=Z_pos)\n",
    "    gp.compute_model(geo_model)\n",
    "\n",
    "\n",
    "def sample_grav(Z_var2):\n",
    "    compute_gempy_model(Z_var2)\n",
    "\n",
    "    # Returns the 3d lith array\n",
    "    return geo_model.solutions.fw_gravity\n",
    "\n",
    "\n",
    "def sample_lith(Z_var):\n",
    "    compute_gempy_model(Z_var)\n",
    "    return geo_model.solutions.block_matrix[0][1]\n",
    "\n",
    "\n",
    "class GemPyGrav(tt.Op):\n",
    "    itypes = [tt.dvector]\n",
    "    otypes = [tt.dvector]\n",
    "\n",
    "    def perform(self, node, inputs, outputs):\n",
    "        theta, = inputs\n",
    "        mu = sample_grav(theta)\n",
    "        outputs[0][0] = np.array(mu)\n",
    "\n",
    "\n",
    "class GemPyLith(tt.Op):\n",
    "    itypes = [tt.dvector]\n",
    "    otypes = [tt.dvector]\n",
    "\n",
    "    def perform(self, node, inputs, outputs):\n",
    "        theta, = inputs\n",
    "   \n",
    "        mu = sample_lith(theta)\n",
    "        outputs[0][0] = np.array(mu)\n",
    "\n",
    "\n",
    "gempy_grav = GemPyGrav()\n",
    "gempy_lith = GemPyLith()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gravity_observations = 1e3 * np.array([-0.34558686, -0.3648658,\n",
    "                                       -0.3448658, -0.3663401,\n",
    "                                       -0.3441554, -0.3634569,\n",
    "                                       -0.3472901, -0.36575098,\n",
    "                                       -0.35575098, -0.36424498,\n",
    "                                       -0.35643718], dtype='float64')\n",
    "\n",
    "thickness_obs = 1e3 * np.array([1.32, 1.16, 1.28, 1.25, 1.08, 1.09,\n",
    "                                1.19, 0.87, 0.96, 0.31, 0.83, 0.92])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with model:\n",
    "    prior = pm.sample_prior_predictive(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = az.from_pymc3(prior=prior)\n",
    "az.plot_density([data.prior], shade=.9, data_labels=[\"Prior\"],\n",
    "                var_names=['depths', 'gravity', 'thick_1', 'thick_2']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with model:\n",
    "    trace = pm.sample(2000, cores=1, chains=1, step=pm.Metropolis(tune_interval=100), tune=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with model:\n",
    "    post = pm.sample_posterior_predictive(trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = az.from_pymc3(trace=trace,\n",
    "                     prior=prior,\n",
    "                     posterior_predictive=post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "az.plot_density([ data.prior, data], shade=.9, data_labels=[ \"Prior\", \"Posterior\"],\n",
    "                var_names=['depths', 'gravity', 'thick_1', 'thick_2']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References  \n",
    "\n",
    "[1] Thomas Wiecki 2015. MCMC sampling for dummies _https://twiecki.io/blog/2015/11/10/mcmc-sampling/_ (last visited 2022-04-18)  \n",
    "[2] Gallagher, K., Charvin, K., Nielsen, S., Sambridge, M., & Stephenson, J. (2009). Markov chain Monte Carlo (MCMC) sampling methods to determine optimal models, model resolution and model choice for Earth Science problems. Marine and Petroleum Geology, 26(4), 525-535.  \n",
    "[3] Salvatier, J., Wiecki, T. V., & Fonnesbeck, C. (2016). Probabilistic programming in Python using PyMC3. PeerJ Computer Science, 2, e55.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[back to top](#0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}